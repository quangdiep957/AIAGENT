# agents.py
import os
from langchain_openai import ChatOpenAI
from langchain.agents import initialize_agent, AgentType
from tools import get_weather, calculate_sum, semantic_search, wiki_search, wiki_summary
import config  # ensure .env is loaded (load_dotenv runs in config)
from langchain.tools import tool
from typing import List, Dict, Any, Optional

# Import cÃ¡c tools hiá»‡n cÃ³
from tools import FileUploadTool, FileReaderTool, OCRTool, EmbeddingTool, VectorSearchTool
from services import EmbeddingService
from database import DatabaseManager

# Khá»Ÿi táº¡o cÃ¡c tools
upload_tool = FileUploadTool(upload_dir="uploads")
reader_tool = FileReaderTool()
ocr_tool = OCRTool()
embedding_tool = EmbeddingTool()
search_tool = VectorSearchTool()
embedding_service = EmbeddingService()
db_manager = DatabaseManager()

@tool
def upload_and_process_document(file_info: str) -> str:
    """
    Upload vÃ  xá»­ lÃ½ tÃ i liá»‡u (PDF, Word, Image) Ä‘á»ƒ lÆ°u vÃ o database vÃ  táº¡o embeddings
    
    Args:
        file_info: ThÃ´ng tin file dáº¡ng "file_path|file_name" (vÃ­ dá»¥: "/path/to/file.pdf|document.pdf")
    
    Returns:
        Káº¿t quáº£ xá»­ lÃ½ file
    """
    try:
        # Parse file info
        if "|" not in file_info:
            return "âŒ Format khÃ´ng Ä‘Ãºng. Vui lÃ²ng sá»­ dá»¥ng format: file_path|file_name"
        
        file_path, file_name = file_info.split("|", 1)
        
        # Upload file
        upload_result = upload_tool.upload_file(
            file_path=file_path,
            metadata={
                "original_name": file_name,
                "upload_source": "ai_agent"
            }
        )
        
        if not upload_result["success"]:
            return f"âŒ Upload tháº¥t báº¡i: {', '.join(upload_result['errors'])}"
        
        file_document = upload_result["document"]
        file_id = upload_result["file_id"]
        
        # Extract content dá»±a trÃªn loáº¡i file
        content = ""
        extraction_method = ""
        file_type = file_document["file_type"]
        file_path_abs = file_document["absolute_path"]
        
        if file_type in ["pdf", "docx", "doc", "txt", "md"]:
            # Äá»c file text-based
            read_result = reader_tool.read_file(file_path_abs)
            if read_result["success"]:
                if file_type == "pdf":
                    content = read_result["total_content"]
                elif file_type in ["docx", "doc"]:
                    content = read_result["total_content"]
                else:  # text files
                    content = read_result["content"]
                extraction_method = "file_reader"
            else:
                return f"âŒ KhÃ´ng thá»ƒ Ä‘á»c file: {read_result['error']}"
        
        elif file_type == "image":
            # OCR cho áº£nh
            ocr_result = ocr_tool.extract_text_from_image(file_path_abs)
            if ocr_result["success"]:
                content = ocr_result["text"]
                extraction_method = "ocr"
            else:
                return f"âŒ OCR tháº¥t báº¡i: {ocr_result['error']}"
        
        if not content or not content.strip():
            return "âŒ KhÃ´ng thá»ƒ trÃ­ch xuáº¥t ná»™i dung tá»« file"
        
        # Táº¡o embeddings
        processing_result = embedding_service.process_file_content(
            file_id=file_id,
            content=content,
            metadata={
                "extraction_method": extraction_method,
                "file_type": file_type
            }
        )
        
        if not processing_result["success"]:
            return f"âŒ Táº¡o embedding tháº¥t báº¡i: {processing_result['error']}"
        
        return f"""âœ… ÄÃ£ xá»­ lÃ½ thÃ nh cÃ´ng file: {file_name}
ğŸ“Š ThÃ´ng tin:
- Loáº¡i file: {file_type.upper()}
- Sá»‘ tá»«: {len(content.split()):,}
- Chá»§ Ä‘á»: {processing_result['topic']}
- Äá»™ khÃ³: {processing_result['difficulty_level']}
- Sá»‘ chunks: {processing_result['total_chunks']}
- Tags: {', '.join(processing_result['tags'])}

TÃ i liá»‡u Ä‘Ã£ Ä‘Æ°á»£c lÆ°u vÃ o database vÃ  sáºµn sÃ ng Ä‘á»ƒ tÃ¬m kiáº¿m!"""
        
    except Exception as e:
        return f"âŒ Lá»—i xá»­ lÃ½ file: {str(e)}"

@tool
def search_documents(query: str) -> str:
    """
    TÃ¬m kiáº¿m tÃ i liá»‡u dá»±a trÃªn ná»™i dung cÃ¢u há»i
    
    Args:
        query: CÃ¢u há»i hoáº·c tá»« khÃ³a cáº§n tÃ¬m
    
    Returns:
        Ná»™i dung tÃ i liá»‡u liÃªn quan Ä‘Æ°á»£c tÃ¬m tháº¥y
    """
    try:
        limit = 3  # Set default limit
        search_result = search_tool.similarity_search(
            query_text=query,
            limit=limit,
            similarity_threshold=0.3
        )
        
        if not search_result["success"]:
            return f"âŒ Lá»—i tÃ¬m kiáº¿m: {search_result['error']}"
        
        if not search_result["results"]:
            return "ğŸ” KhÃ´ng tÃ¬m tháº¥y tÃ i liá»‡u nÃ o liÃªn quan Ä‘áº¿n cÃ¢u há»i nÃ y. Báº¡n cÃ³ thá»ƒ upload thÃªm tÃ i liá»‡u Ä‘á»ƒ tÃ´i há»— trá»£ tá»‘t hÆ¡n."
        
        # Format káº¿t quáº£ tÃ¬m kiáº¿m
        results = []
        for i, doc in enumerate(search_result["results"], 1):
            # Giá»¯ nguyÃªn ná»™i dung tiáº¿ng Anh, chá»‰ thÃªm thÃ´ng tin metadata báº±ng tiáº¿ng Viá»‡t
            content_preview = doc['content'][:500]
            if len(doc['content']) > 500:
                content_preview += "..."
            
            results.append(f"""
ğŸ“„ **TÃ i liá»‡u {i}** (Äá»™ tÆ°Æ¡ng Ä‘á»“ng: {doc['similarity_score']:.2f})
ğŸ“š Chá»§ Ä‘á»: {doc.get('topic', 'N/A')}
ğŸ“ Ná»™i dung: {content_preview}
""")
        
        return f"""ğŸ” **TÃ¬m tháº¥y {len(search_result['results'])} tÃ i liá»‡u liÃªn quan:**

{''.join(results)}

ğŸ’¡ Dá»±a trÃªn nhá»¯ng tÃ i liá»‡u tiáº¿ng Anh trÃªn, tÃ´i cÃ³ thá»ƒ giÃºp báº¡n há»c táº­p hiá»‡u quáº£."""
        
    except Exception as e:
        return f"âŒ Lá»—i tÃ¬m kiáº¿m: {str(e)}"

@tool
def get_document_summary() -> str:
    """
    Láº¥y tÃ³m táº¯t vá» cÃ¡c tÃ i liá»‡u Ä‘Ã£ upload trong database
    
    Returns:
        Thá»‘ng kÃª tÃ i liá»‡u trong database
    """
    try:
        collections = db_manager.get_collections()
        
        if "document_embeddings" in collections:
            collection = db_manager.db["document_embeddings"]
            total_docs = collection.count_documents({})
            
            # Láº¥y thá»‘ng kÃª theo chá»§ Ä‘á»
            pipeline = [
                {"$group": {"_id": "$topic", "count": {"$sum": 1}}},
                {"$sort": {"count": -1}},
                {"$limit": 5}
            ]
            topics = list(collection.aggregate(pipeline))
            
            topic_summary = "\n".join([f"- {topic['_id']}: {topic['count']} tÃ i liá»‡u" for topic in topics])
            
            return f"""ğŸ“Š **Thá»‘ng kÃª tÃ i liá»‡u trong database:**

ğŸ“š Tá»•ng sá»‘ tÃ i liá»‡u: {total_docs}

ğŸ” **Top chá»§ Ä‘á»:**
{topic_summary}

ğŸ’¡ Báº¡n cÃ³ thá»ƒ há»i tÃ´i vá» báº¥t ká»³ chá»§ Ä‘á» nÃ o trong danh sÃ¡ch trÃªn!"""
        else:
            return "ğŸ“š ChÆ°a cÃ³ tÃ i liá»‡u nÃ o Ä‘Æ°á»£c upload. HÃ£y upload tÃ i liá»‡u Ä‘á»ƒ tÃ´i cÃ³ thá»ƒ há»— trá»£ báº¡n!"
            
    except Exception as e:
        return f"âŒ Lá»—i láº¥y thá»‘ng kÃª: {str(e)}"

def get_llm():
    """Khá»Ÿi táº¡o ChatOpenAI model"""
    return ChatOpenAI(model="gpt-4.1", temperature=0.3)

def create_agent():
    """Táº¡o AI agent vá»›i cÃ¡c tools tÃ­ch há»£p"""
    llm = get_llm()
    # Æ¯u tiÃªn semantic_search Ä‘á»©ng trÆ°á»›c wiki_search
    tools = [get_weather, calculate_sum, semantic_search, wiki_search, wiki_summary,upload_and_process_document,search_documents,get_document_summary]

    agent = initialize_agent(
        tools,
        llm,
        agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,
        verbose=True,
        handle_parsing_errors=True
    )
    return agent
