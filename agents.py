# agents.py
from langchain_openai import ChatOpenAI
from langchain.agents import initialize_agent, AgentType
from tools import get_weather, calculate_sum, semantic_search, wiki_search, wiki_summary
import config  # ensure .env is loaded (load_dotenv runs in config)
from langchain.tools import tool
from typing import List, Dict, Any, Optional
import json
from datetime import datetime

# Import c√°c tools hi·ªán c√≥
from tools import FileUploadTool, FileReaderTool, OCRTool, EmbeddingTool, VectorSearchTool
from tools import save_chat_content, get_chat_history_summary, search_chat_and_documents, auto_save_english_content
from tools import search_web_with_evaluation, generate_llm_response_for_query
from services import EmbeddingService
from database import DatabaseManager

# Kh·ªüi t·∫°o c√°c tools
upload_tool = FileUploadTool(upload_dir="uploads")
reader_tool = FileReaderTool()
ocr_tool = OCRTool()
embedding_tool = EmbeddingTool()
search_tool = VectorSearchTool()
embedding_service = EmbeddingService()
db_manager = DatabaseManager()


@tool
def upload_and_process_document(file_info: str) -> str:
    """
    Upload v√† x·ª≠ l√Ω t√†i li·ªáu (PDF, Word, Image) ƒë·ªÉ l∆∞u v√†o database v√† t·∫°o embeddings

    Args:
        file_info: Th√¥ng tin file d·∫°ng "file_path|file_name" (v√≠ d·ª•: "/path/to/file.pdf|document.pdf")

    Returns:
        K·∫øt qu·∫£ x·ª≠ l√Ω file
    """
    try:
        # Parse file info
        if "|" not in file_info:
            return "‚ùå Format kh√¥ng ƒë√∫ng. Vui l√≤ng s·ª≠ d·ª•ng format: file_path|file_name"

        file_path, file_name = file_info.split("|", 1)

        # Upload file
        upload_result = upload_tool.upload_file(
            file_path=file_path,
            metadata={
                "original_name": file_name,
                "upload_source": "ai_agent"
            }
        )

        if not upload_result["success"]:
            return f"‚ùå Upload th·∫•t b·∫°i: {', '.join(upload_result['errors'])}"

        file_document = upload_result["document"]
        file_id = upload_result["file_id"]

        # Extract content d·ª±a tr√™n lo·∫°i file
        content = ""
        extraction_method = ""
        file_type = file_document["file_type"]
        file_path_abs = file_document["absolute_path"]

        if file_type in ["pdf", "docx", "doc", "txt", "md"]:
            # ƒê·ªçc file text-based
            read_result = reader_tool.read_file(file_path_abs)
            if read_result["success"]:
                if file_type == "pdf":
                    content = read_result["total_content"]
                elif file_type in ["docx", "doc"]:
                    content = read_result["total_content"]
                else:  # text files
                    content = read_result["content"]
                extraction_method = "file_reader"
            else:
                return f"‚ùå Kh√¥ng th·ªÉ ƒë·ªçc file: {read_result['error']}"

        elif file_type == "image":
            # OCR cho ·∫£nh
            ocr_result = ocr_tool.extract_text_from_image(file_path_abs)
            if ocr_result["success"]:
                content = ocr_result["text"]
                extraction_method = "ocr"
            else:
                return f"‚ùå OCR th·∫•t b·∫°i: {ocr_result['error']}"

        if not content or not content.strip():
            return "‚ùå Kh√¥ng th·ªÉ tr√≠ch xu·∫•t n·ªôi dung t·ª´ file"

        # T·∫°o embeddings
        processing_result = embedding_service.process_file_content(
            file_id=file_id,
            content=content,
            metadata={
                "extraction_method": extraction_method,
                "file_type": file_type
            }
        )

        if not processing_result["success"]:
            return f"‚ùå T·∫°o embedding th·∫•t b·∫°i: {processing_result['error']}"

        return f"""‚úÖ ƒê√£ x·ª≠ l√Ω th√†nh c√¥ng file: {file_name}
üìä Th√¥ng tin:
- Lo·∫°i file: {file_type.upper()}
- S·ªë t·ª´: {len(content.split()):,}
- Ch·ªß ƒë·ªÅ: {processing_result['topic']}
- ƒê·ªô kh√≥: {processing_result['difficulty_level']}
- S·ªë chunks: {processing_result['total_chunks']}
- Tags: {', '.join(processing_result['tags'])}

T√†i li·ªáu ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o database v√† s·∫µn s√†ng ƒë·ªÉ t√¨m ki·∫øm!"""

    except Exception as e:
        return f"‚ùå L·ªói x·ª≠ l√Ω file: {str(e)}"


@tool
def search_documents(query: str) -> str:
    """
    T√¨m ki·∫øm t√†i li·ªáu d·ª±a tr√™n n·ªôi dung c√¢u h·ªèi

    Args:
        query: C√¢u h·ªèi ho·∫∑c t·ª´ kh√≥a c·∫ßn t√¨m

    Returns:
        N·ªôi dung t√†i li·ªáu li√™n quan ƒë∆∞·ª£c t√¨m th·∫•y
    """
    try:
        limit = 3  # Set default limit
        search_result = search_tool.similarity_search(
            query_text=query,
            limit=limit,
            similarity_threshold=0.3
        )

        if not search_result["success"]:
            return f"‚ùå L·ªói t√¨m ki·∫øm: {search_result['error']}"

        if not search_result["results"]:
            return "üîç Kh√¥ng t√¨m th·∫•y t√†i li·ªáu n√†o li√™n quan ƒë·∫øn c√¢u h·ªèi n√†y. B·∫°n c√≥ th·ªÉ upload th√™m t√†i li·ªáu ƒë·ªÉ t√¥i h·ªó tr·ª£ t·ªët h∆°n."

        # Format k·∫øt qu·∫£ t√¨m ki·∫øm
        results = []
        for i, doc in enumerate(search_result["results"], 1):
            # Gi·ªØ nguy√™n n·ªôi dung ti·∫øng Anh, ch·ªâ th√™m th√¥ng tin metadata b·∫±ng ti·∫øng Vi·ªát
            content_preview = doc['content'][:500]
            if len(doc['content']) > 500:
                content_preview += "..."

            results.append(f"""
üìÑ **T√†i li·ªáu {i}** (ƒê·ªô t∆∞∆°ng ƒë·ªìng: {doc['similarity_score']:.2f})
üìö Ch·ªß ƒë·ªÅ: {doc.get('topic', 'N/A')}
üìù N·ªôi dung: {content_preview}
""")

        return f"""üîç **T√¨m th·∫•y {len(search_result['results'])} t√†i li·ªáu li√™n quan:**

{''.join(results)}

üí° D·ª±a tr√™n nh·ªØng t√†i li·ªáu ti·∫øng Anh tr√™n, t√¥i c√≥ th·ªÉ gi√∫p b·∫°n h·ªçc t·∫≠p hi·ªáu qu·∫£."""

    except Exception as e:
        return f"‚ùå L·ªói t√¨m ki·∫øm: {str(e)}"


@tool
def get_document_summary(dummy_input: str = "") -> str:
    """
    L·∫•y t√≥m t·∫Øt v·ªÅ c√°c t√†i li·ªáu ƒë√£ upload trong database
    
    Returns:
        Th·ªëng k√™ t√†i li·ªáu trong database
    """
    try:
        collections = db_manager.get_collections()

        if "document_embeddings" in collections:
            collection = db_manager.db["document_embeddings"]
            total_docs = collection.count_documents({})

            # L·∫•y th·ªëng k√™ theo ch·ªß ƒë·ªÅ
            pipeline = [
                {"$group": {"_id": "$topic", "count": {"$sum": 1}}},
                {"$sort": {"count": -1}},
                {"$limit": 5}
            ]
            topics = list(collection.aggregate(pipeline))

            topic_summary = "\n".join([f"- {topic['_id']}: {topic['count']} t√†i li·ªáu" for topic in topics])

            return f"""üìä **Th·ªëng k√™ t√†i li·ªáu trong database:**

üìö T·ªïng s·ªë t√†i li·ªáu: {total_docs}

üîç **Top ch·ªß ƒë·ªÅ:**
{topic_summary}

üí° B·∫°n c√≥ th·ªÉ h·ªèi t√¥i v·ªÅ b·∫•t k·ª≥ ch·ªß ƒë·ªÅ n√†o trong danh s√°ch tr√™n!"""
        else:
            return "üìö Ch∆∞a c√≥ t√†i li·ªáu n√†o ƒë∆∞·ª£c upload. H√£y upload t√†i li·ªáu ƒë·ªÉ t√¥i c√≥ th·ªÉ h·ªó tr·ª£ b·∫°n!"

    except Exception as e:
        return f"‚ùå L·ªói l·∫•y th·ªëng k√™: {str(e)}"

@tool
def smart_search_and_answer(query: str) -> str:
    """
    T√¨m ki·∫øm th√¥ng minh: Knowledge Base -> Web Search -> LLM Response
    
    Args:
        query: C√¢u h·ªèi ho·∫∑c t·ª´ kh√≥a c·∫ßn t√¨m
    
    Returns:
        K·∫øt qu·∫£ t√¨m ki·∫øm v√† tr·∫£ l·ªùi t·ªëi ∆∞u
    """
    try:
        # B∆∞·ªõc 1: T√¨m trong Knowledge Base tr∆∞·ªõc
        kb_results = search_tool.similarity_search(
            query_text=query,
            limit=3,
            similarity_threshold=0.4  # Ng∆∞·ª°ng cao h∆°n ƒë·ªÉ ƒë·∫£m b·∫£o ch·∫•t l∆∞·ª£ng
        )
        
        if kb_results["success"] and kb_results["results"]:
            # C√≥ k·∫øt qu·∫£ trong KB, ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng
            best_score = max(result['similarity_score'] for result in kb_results["results"])
            
            if best_score >= 0.6:  # K·∫øt qu·∫£ KB t·ªët
                results_text = []
                for i, doc in enumerate(kb_results["results"], 1):
                    content_preview = doc['content'][:400] + "..." if len(doc['content']) > 400 else doc['content']
                    results_text.append(f"""
üìÑ **T√†i li·ªáu {i}** (ƒê·ªô t∆∞∆°ng ƒë·ªìng: {doc['similarity_score']:.2f})
üìö Ch·ªß ƒë·ªÅ: {doc.get('topic', 'N/A')}
üìù N·ªôi dung: {content_preview}
""")
                
                return f"""‚úÖ **T√¨m th·∫•y trong Knowledge Base**

üîç **T√¨m th·∫•y {len(kb_results['results'])} t√†i li·ªáu li√™n quan:**

{''.join(results_text)}

üí° **Ngu·ªìn:** Knowledge Base ch·∫•t l∆∞·ª£ng cao
**Tr·∫°ng th√°i:** knowledge_base_found"""
        
        # B∆∞·ªõc 2: KB kh√¥ng c√≥ ho·∫∑c ch·∫•t l∆∞·ª£ng th·∫•p -> T√¨m ki·∫øm web
        from tools.web_search_tool import search_web_with_evaluation, generate_llm_response_for_query
        web_result = search_web_with_evaluation(query)
        
        if "search_results_ready" in web_result:
            return web_result
        elif "llm_response_needed" in web_result:
            # B∆∞·ªõc 3: Web search kh√¥ng t·ªët -> D√πng LLM
            llm_response = generate_llm_response_for_query(query)
            return llm_response
        else:
            # Fallback
            return web_result
            
    except Exception as e:
        # Final fallback
        from tools.web_search_tool import generate_llm_response_for_query
        llm_response = generate_llm_response_for_query(query)
        return f"""‚ö†Ô∏è **L·ªói trong qu√° tr√¨nh t√¨m ki·∫øm: {str(e)}**

{llm_response}"""

def get_llm():
    """Kh·ªüi t·∫°o ChatOpenAI model"""
    return ChatOpenAI(model="gpt-4.1", temperature=0.3)


def create_agent():
    """T·∫°o AI agent v·ªõi c√°c tools t√≠ch h·ª£p (ƒë·∫ßu v√†o ƒë∆°n gi·∫£n 1 tham s·ªë)"""
    llm = get_llm()
    # Ch·ªâ d√πng c√°c tool 1 tham s·ªë ƒë·ªÉ t∆∞∆°ng th√≠ch ZeroShotAgent
    tools = [
        get_weather,
        calculate_sum,
        semantic_search,
        wiki_search,
        wiki_summary,
        upload_and_process_document,
        search_documents,
        get_document_summary,
        plan_study_auto,
        plan_study_schedule,
        smart_search_and_answer,
        search_web_with_evaluation,
        generate_llm_response_for_query
    ]


    agent_instructions = (
        "B·∫°n l√† tr·ª£ l√Ω h·ªçc t·∫≠p. Khi ng∆∞·ªùi d√πng c√≥ √Ω ƒë·ªãnh xin 'l·ªô tr√¨nh', 'k·∫ø ho·∫°ch', 'l·ªãch h·ªçc' ho·∫∑c t∆∞∆°ng t·ª±, "
        "h√£y G·ªåI c√¥ng c·ª• plan_study_auto v√† truy·ªÅn NGUY√äN VƒÇN c√¢u h·ªèi l√†m ƒë·∫ßu v√†o.\n"
        "Khi c·∫ßn ki·∫øn th·ª©c tham chi·∫øu, LU√îN ∆∞u ti√™n g·ªçi semantic_search tr∆∞·ªõc ƒë·ªÉ t√¨m trong t√†i li·ªáu ng∆∞·ªùi d√πng; "
        "ch·ªâ g·ªçi wiki_search n·∫øu semantic_search tr·∫£ v·ªÅ 'NO_HITS'."
    )

    # T·∫°o agent ƒë∆°n gi·∫£n v·ªõi ReAct
    agent = initialize_agent(
        tools,
        llm,
        agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,
        verbose=True,
        handle_parsing_errors=True,
        agent_kwargs={
            "system_message": agent_instructions
        },
        handle_parsing_errors=True
    )
    return agent
