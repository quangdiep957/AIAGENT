"""
Chat Knowledge Tool - Tool ƒë·ªÉ qu·∫£n l√Ω l·ªãch s·ª≠ chat v√† knowledge base
"""

from langchain.tools import tool
from typing import Dict, Any
import json
from datetime import datetime
import tempfile
import os

from database import DatabaseManager
from services.embedding_service import EmbeddingService

# Kh·ªüi t·∫°o services
db_manager = DatabaseManager()
embedding_service = EmbeddingService()

@tool  
def save_chat_content(content: str) -> str:
    """
    L∆∞u n·ªôi dung ti·∫øng Anh t·ª´ chat v√†o knowledge base
    
    Args:
        content: N·ªôi dung c·∫ßn l∆∞u (vƒÉn b·∫£n ti·∫øng Anh)
    
    Returns:
        K·∫øt qu·∫£ l∆∞u tr·ªØ
    """
    try:
        # Ki·ªÉm tra n·ªôi dung c√≥ ph·∫£i ti·∫øng Anh kh√¥ng
        english_indicators = ['the', 'and', 'of', 'to', 'a', 'in', 'is', 'it', 'you', 'that', 'he', 'was', 'for', 'on', 'are', 'as', 'with', 'his', 'they']
        content_lower = content.lower()
        english_count = sum(1 for word in english_indicators if word in content_lower)
        
        if english_count < 3 or len(content.split()) < 10:
            return "‚è≠Ô∏è N·ªôi dung qu√° ng·∫Øn ho·∫∑c kh√¥ng ph·∫£i ti·∫øng Anh, b·ªè qua l∆∞u tr·ªØ"
        
        # T·∫°o metadata cho chat content
        chat_data = {
            "content": content,
            "source": "chat_conversation",
            "timestamp": datetime.now().isoformat(),
            "word_count": len(content.split()),
            "type": "chat_content"
        }
        
        # L∆∞u v√†o collection ƒë·∫∑c bi·ªát cho chat
        collection = db_manager.db["chat_knowledge"]
        result = collection.insert_one(chat_data)
        
        # T·∫°o embedding cho n·ªôi dung
        embedding_result = embedding_service.create_embedding_for_text(
            text=content,
            metadata={
                "source": "chat",
                "chat_id": str(result.inserted_id),
                "timestamp": chat_data["timestamp"]
            }
        )
        
        if embedding_result["success"]:
            return f"‚úÖ ƒê√£ l∆∞u n·ªôi dung ti·∫øng Anh v√†o knowledge base ({len(content.split())} t·ª´)"
        else:
            return f"‚ö†Ô∏è ƒê√£ l∆∞u n·ªôi dung nh∆∞ng kh√¥ng t·∫°o ƒë∆∞·ª£c embedding: {embedding_result['error']}"
            
    except Exception as e:
        return f"‚ùå L·ªói l∆∞u chat content: {str(e)}"

@tool
def get_chat_history_summary(session_name: str = "current") -> str:
    """
    L·∫•y t√≥m t·∫Øt l·ªãch s·ª≠ chat t·ª´ database
    
    Args:
        session_name: T√™n session chat c·∫ßn l·∫•y l·ªãch s·ª≠
    
    Returns:
        T√≥m t·∫Øt l·ªãch s·ª≠ chat
    """
    try:
        collection = db_manager.db["chat_knowledge"]
        
        # L·∫•y chat g·∫ßn ƒë√¢y
        recent_chats = list(collection.find(
            {"source": "chat_conversation"},
            {"content": 1, "timestamp": 1, "word_count": 1}
        ).sort("timestamp", -1).limit(20))
        
        if not recent_chats:
            return "üìù Ch∆∞a c√≥ l·ªãch s·ª≠ chat n√†o ƒë∆∞·ª£c l∆∞u trong knowledge base"
        
        total_words = sum(chat.get("word_count", 0) for chat in recent_chats)
        
        # T·∫°o t√≥m t·∫Øt
        summary = f"""üìö **T√≥m t·∫Øt l·ªãch s·ª≠ chat:**

üî¢ S·ªë ƒëo·∫°n chat ƒë√£ l∆∞u: {len(recent_chats)}
üìù T·ªïng s·ªë t·ª´: {total_words:,}
üïê Chat g·∫ßn nh·∫•t: {recent_chats[0]['timestamp'][:19] if recent_chats else 'N/A'}

üí° C√°c n·ªôi dung n√†y ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o knowledge base v√† c√≥ th·ªÉ ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi!"""

        return summary
        
    except Exception as e:
        return f"‚ùå L·ªói l·∫•y l·ªãch s·ª≠ chat: {str(e)}"

@tool
def search_chat_and_documents(query: str) -> str:
    """
    T√¨m ki·∫øm th√¥ng tin t·ª´ c·∫£ l·ªãch s·ª≠ chat v√† t√†i li·ªáu trong knowledge base
    
    Args:
        query: C√¢u h·ªèi ho·∫∑c t·ª´ kh√≥a c·∫ßn t√¨m
    
    Returns:
        K·∫øt qu·∫£ t√¨m ki·∫øm t·ª´ nhi·ªÅu ngu·ªìn
    """
    try:
        from tools.vector_search_tool import VectorSearchTool
        search_tool = VectorSearchTool()
        
        # T√¨m ki·∫øm trong documents (tool c√≥ s·∫µn)
        doc_results = search_tool.similarity_search(
            query_text=query,
            limit=2,
            similarity_threshold=0.3
        )
        
        # T√¨m ki·∫øm trong chat history
        chat_collection = db_manager.db["chat_knowledge"]
        
        # T·∫°o text index n·∫øu ch∆∞a c√≥
        try:
            chat_collection.create_index([("content", "text")])
        except:
            pass  # Index ƒë√£ t·ªìn t·∫°i
        
        chat_docs = list(chat_collection.find(
            {"$text": {"$search": query}},
            {"content": 1, "timestamp": 1}
        ).limit(3))
        
        # K·∫øt h·ª£p k·∫øt qu·∫£
        results = []
        
        # Th√™m k·∫øt qu·∫£ t·ª´ documents
        if doc_results["success"] and doc_results["results"]:
            results.append("üîç **T·ª´ t√†i li·ªáu ƒë√£ upload:**")
            for i, doc in enumerate(doc_results["results"], 1):
                content_preview = doc['content'][:300] + "..." if len(doc['content']) > 300 else doc['content']
                results.append(f"""
üìÑ **T√†i li·ªáu {i}** (ƒê·ªô t∆∞∆°ng ƒë·ªìng: {doc['similarity_score']:.2f})
üìö Ch·ªß ƒë·ªÅ: {doc.get('topic', 'N/A')}
üìù N·ªôi dung: {content_preview}
""")
        
        # Th√™m k·∫øt qu·∫£ t·ª´ chat history  
        if chat_docs:
            results.append("\nüí¨ **T·ª´ l·ªãch s·ª≠ chat:**")
            for i, chat in enumerate(chat_docs, 1):
                content_preview = chat['content'][:300] + "..." if len(chat['content']) > 300 else chat['content']
                results.append(f"""
üó®Ô∏è **Chat {i}** ({chat['timestamp'][:19]})
üìù N·ªôi dung: {content_preview}
""")
        
        if not results:
            return "üîç Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan trong c·∫£ t√†i li·ªáu v√† l·ªãch s·ª≠ chat. H√£y th·ª≠ t·ª´ kh√≥a kh√°c!"
        
        return "\n".join(results) + "\n\nüí° D·ª±a tr√™n th√¥ng tin tr√™n, t√¥i c√≥ th·ªÉ gi√∫p b·∫°n tr·∫£ l·ªùi c√¢u h·ªèi chi ti·∫øt h∆°n."
        
    except Exception as e:
        return f"‚ùå L·ªói t√¨m ki·∫øm: {str(e)}"

@tool
def auto_save_english_content(content_with_session: str) -> str:
    """
    T·ª± ƒë·ªông ph√°t hi·ªán v√† l∆∞u n·ªôi dung ti·∫øng Anh v√†o knowledge base
    
    Args:
        content_with_session: N·ªôi dung c·∫ßn l∆∞u, format: "content|session_name" ho·∫∑c ch·ªâ "content"
    
    Returns:
        K·∫øt qu·∫£ x·ª≠ l√Ω
    """
    try:
        # Parse input
        if "|" in content_with_session:
            content, session_name = content_with_session.split("|", 1)
        else:
            content = content_with_session
            session_name = "default"
        # Ki·ªÉm tra xem c√≥ ph·∫£i ti·∫øng Anh kh√¥ng (ƒë∆°n gi·∫£n)
        english_indicators = ['the', 'and', 'of', 'to', 'a', 'in', 'is', 'it', 'you', 'that', 'he', 'was', 'for', 'on', 'are', 'as', 'with', 'his', 'they']
        content_lower = content.lower()
        english_count = sum(1 for word in english_indicators if word in content_lower)
        
        if english_count >= 3 and len(content.split()) >= 10:  # C√≥ √≠t nh·∫•t 3 t·ª´ ti·∫øng Anh v√† 10 t·ª´
            # T·∫°o file t·∫°m v√† upload v√†o knowledge base
            temp_file = tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False, encoding='utf-8')
            temp_file.write(content)
            temp_file.close()
            
            # Import tool ƒë·ªÉ x·ª≠ l√Ω file
            from tools.file_upload_tool import FileUploadTool
            from tools.file_reader_tool import FileReaderTool
            
            upload_tool = FileUploadTool(upload_dir="uploads")
            reader_tool = FileReaderTool()
            
            # Upload file
            upload_result = upload_tool.upload_file(
                file_path=temp_file.name,
                metadata={
                    "original_name": f"chat_content_{session_name}.txt",
                    "upload_source": "auto_chat_save",
                    "session": session_name
                }
            )
            
            # Cleanup temp file
            os.unlink(temp_file.name)
            
            if upload_result["success"]:
                file_id = upload_result["file_id"]
                
                # T·∫°o embeddings
                processing_result = embedding_service.process_file_content(
                    file_id=file_id,
                    content=content,
                    metadata={
                        "extraction_method": "auto_chat_save",
                        "file_type": "chat_text",
                        "session": session_name
                    }
                )
                
                if processing_result["success"]:
                    return f"‚úÖ ƒê√£ t·ª± ƒë·ªông l∆∞u n·ªôi dung ti·∫øng Anh v√†o knowledge base ({len(content.split())} t·ª´)"
                else:
                    return f"‚ö†Ô∏è ƒê√£ l∆∞u file nh∆∞ng kh√¥ng t·∫°o ƒë∆∞·ª£c embedding: {processing_result['error']}"
            else:
                return f"‚ùå Kh√¥ng th·ªÉ l∆∞u content: {', '.join(upload_result['errors'])}"
        else:
            return "‚è≠Ô∏è N·ªôi dung kh√¥ng ƒë·ªß ti√™u ch√≠ ƒë·ªÉ l∆∞u v√†o knowledge base"
            
    except Exception as e:
        return f"‚ùå L·ªói auto save: {str(e)}"
