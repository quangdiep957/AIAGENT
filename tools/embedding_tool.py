"""
Embedding Tool - T·∫°o vector embeddings t·ª´ text s·ª≠ d·ª•ng OpenAI API
H·ªó tr·ª£ chunking text v√† batch processing
"""

import os
import re
import time
from typing import Dict, List, Any, Optional, Union
from datetime import datetime
import hashlib
import tiktoken
from openai import OpenAI
from config import OPENAI_API_KEY

class EmbeddingTool:
    """Tool t·∫°o vector embeddings t·ª´ text"""
    
    # C√°c model embedding c·ªßa OpenAI
    EMBEDDING_MODELS = {
        "text-embedding-3-small": {
            "dimensions": 1536,
            "max_tokens": 8192,
            "cost_per_1k": 0.00002  # USD
        },
        "text-embedding-3-large": {
            "dimensions": 3072,
            "max_tokens": 8192,
            "cost_per_1k": 0.00013  # USD
        },
        "text-embedding-ada-002": {
            "dimensions": 1536,
            "max_tokens": 8192,
            "cost_per_1k": 0.0001  # USD
        }
    }
    
    def __init__(self, model: str = "text-embedding-3-small", api_key: Optional[str] = None):
        """
        Kh·ªüi t·∫°o EmbeddingTool
        
        Args:
            model (str): T√™n model embedding
            api_key (Optional[str]): OpenAI API key
        """
        self.model = model
        self.api_key = api_key or OPENAI_API_KEY
        
        if not self.api_key:
            raise ValueError("OpenAI API key kh√¥ng ƒë∆∞·ª£c cung c·∫•p")
        
        # Kh·ªüi t·∫°o OpenAI client
        self.client = OpenAI(api_key=self.api_key)
        
        # Kh·ªüi t·∫°o tokenizer
        try:
            self.tokenizer = tiktoken.get_encoding("cl100k_base")
        except Exception as e:
            print(f"Kh√¥ng th·ªÉ kh·ªüi t·∫°o tokenizer: {e}")
            self.tokenizer = None
        
        # Validate model
        if model not in self.EMBEDDING_MODELS:
            raise ValueError(f"Model '{model}' kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£. C√°c model h·ªó tr·ª£: {list(self.EMBEDDING_MODELS.keys())}")
        
        self.model_info = self.EMBEDDING_MODELS[model]
        
        # Th·ªëng k√™ usage
        self.usage_stats = {
            "total_tokens": 0,
            "total_requests": 0,
            "total_cost": 0.0
        }
    
    def _count_tokens(self, text: str) -> int:
        """
        ƒê·∫øm s·ªë tokens trong text
        
        Args:
            text (str): Text c·∫ßn ƒë·∫øm
            
        Returns:
            int: S·ªë tokens
        """
        if self.tokenizer:
            try:
                return len(self.tokenizer.encode(text))
            except:
                pass
        
        # Fallback: ∆∞·ªõc t√≠nh d·ª±a tr√™n s·ªë t·ª´
        return len(text.split()) * 1.3  # Rough estimate
    
    def _clean_text(self, text: str) -> str:
        """
        L√†m s·∫°ch text tr∆∞·ªõc khi embedding
        
        Args:
            text (str): Text c·∫ßn l√†m s·∫°ch
            
        Returns:
            str: Text ƒë√£ ƒë∆∞·ª£c l√†m s·∫°ch
        """
        if not text:
            return ""
        
        # Lo·∫°i b·ªè k√Ω t·ª± ƒëi·ªÅu khi·ªÉn
        text = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f-\x84\x86-\x9f]', '', text)
        
        # Chu·∫©n h√≥a kho·∫£ng tr·∫Øng
        text = re.sub(r'\s+', ' ', text)
        
        # Lo·∫°i b·ªè kho·∫£ng tr·∫Øng ƒë·∫ßu/cu·ªëi
        text = text.strip()
        
        return text
    
    def _split_text_by_tokens(self, text: str, max_tokens: int = None, overlap_tokens: int = 50) -> List[str]:
        """
        Chia text th√†nh chunks d·ª±a tr√™n s·ªë tokens
        
        Args:
            text (str): Text c·∫ßn chia
            max_tokens (int): S·ªë tokens t·ªëi ƒëa m·ªói chunk
            overlap_tokens (int): S·ªë tokens overlap gi·ªØa c√°c chunk
            
        Returns:
            List[str]: Danh s√°ch chunks
        """
        if max_tokens is None:
            max_tokens = self.model_info["max_tokens"] - 100  # ƒê·ªÉ l·∫°i buffer
        
        if not self.tokenizer:
            # Fallback: chia theo s·ªë k√Ω t·ª±
            return self._split_text_by_chars(text, max_tokens * 4, overlap_tokens * 4)
        
        try:
            tokens = self.tokenizer.encode(text)
            
            if len(tokens) <= max_tokens:
                return [text]
            
            chunks = []
            start = 0
            
            while start < len(tokens):
                end = start + max_tokens
                chunk_tokens = tokens[start:end]
                
                # Decode v·ªÅ text
                chunk_text = self.tokenizer.decode(chunk_tokens)
                chunks.append(chunk_text)
                
                # Di chuy·ªÉn start v·ªõi overlap
                start = end - overlap_tokens
            
            return chunks
            
        except Exception as e:
            print(f"L·ªói khi chia text theo tokens: {e}")
            # Fallback
            return self._split_text_by_chars(text, max_tokens * 4, overlap_tokens * 4)
    
    def _split_text_by_chars(self, text: str, max_chars: int = 3000, overlap_chars: int = 200) -> List[str]:
        """
        Chia text th√†nh chunks d·ª±a tr√™n s·ªë k√Ω t·ª±
        
        Args:
            text (str): Text c·∫ßn chia
            max_chars (int): S·ªë k√Ω t·ª± t·ªëi ƒëa m·ªói chunk
            overlap_chars (int): S·ªë k√Ω t·ª± overlap
            
        Returns:
            List[str]: Danh s√°ch chunks
        """
        if len(text) <= max_chars:
            return [text]
        
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + max_chars
            
            # N·∫øu kh√¥ng ph·∫£i chunk cu·ªëi, t√¨m ƒëi·ªÉm c·∫Øt h·ª£p l√Ω
            if end < len(text):
                # T√¨m d·∫•u c√¢u g·∫ßn nh·∫•t
                for i in range(end, max(start + max_chars - overlap_chars, end - 200), -1):
                    if text[i] in '.!?;:\n':
                        end = i + 1
                        break
            
            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)
            
            start = end - overlap_chars if end < len(text) else end
        
        return chunks
    
    def create_embedding(self, text: str, normalize: bool = True) -> Dict[str, Any]:
        """
        T·∫°o embedding cho m·ªôt ƒëo·∫°n text
        
        Args:
            text (str): Text c·∫ßn t·∫°o embedding
            normalize (bool): C√≥ normalize vector kh√¥ng
            
        Returns:
            Dict[str, Any]: K·∫øt qu·∫£ embedding
        """
        try:
            # L√†m s·∫°ch text
            clean_text = self._clean_text(text)
            if not clean_text:
                return {
                    "success": False,
                    "error": "Text r·ªóng sau khi l√†m s·∫°ch"
                }
            
            # ƒê·∫øm tokens
            token_count = self._count_tokens(clean_text)
            
            # Ki·ªÉm tra gi·ªõi h·∫°n tokens
            if token_count > self.model_info["max_tokens"]:
                return {
                    "success": False,
                    "error": f"Text qu√° d√†i ({token_count} tokens). Gi·ªõi h·∫°n: {self.model_info['max_tokens']} tokens"
                }
            
            # G·ªçi OpenAI API
            response = self.client.embeddings.create(
                input=clean_text,
                model=self.model
            )
            
            # L·∫•y embedding vector
            embedding = response.data[0].embedding
            
            # Normalize vector n·∫øu c·∫ßn
            if normalize:
                import math
                magnitude = math.sqrt(sum(x**2 for x in embedding))
                if magnitude > 0:
                    embedding = [x / magnitude for x in embedding]
            
            # C·∫≠p nh·∫≠t usage stats
            self.usage_stats["total_tokens"] += token_count
            self.usage_stats["total_requests"] += 1
            self.usage_stats["total_cost"] += (token_count / 1000) * self.model_info["cost_per_1k"]
            
            return {
                "success": True,
                "embedding": embedding,
                "dimensions": len(embedding),
                "token_count": token_count,
                "model": self.model,
                "text_length": len(clean_text),
                "created_at": datetime.utcnow()
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": f"L·ªói khi t·∫°o embedding: {str(e)}"
            }
    
    def create_embeddings_batch(self, texts: List[str], batch_size: int = 100) -> Dict[str, Any]:
        """
        T·∫°o embeddings cho nhi·ªÅu text c√πng l√∫c
        
        Args:
            texts (List[str]): Danh s√°ch texts
            batch_size (int): S·ªë text x·ª≠ l√Ω m·ªói batch
            
        Returns:
            Dict[str, Any]: K·∫øt qu·∫£ embeddings
        """
        try:
            if not texts:
                return {
                    "success": False,
                    "error": "Danh s√°ch texts r·ªóng"
                }
            
            all_embeddings = []
            total_tokens = 0
            failed_indices = []
            
            # X·ª≠ l√Ω t·ª´ng batch
            for i in range(0, len(texts), batch_size):
                batch_texts = texts[i:i + batch_size]
                
                # L√†m s·∫°ch texts
                clean_texts = [self._clean_text(text) for text in batch_texts]
                
                # L·ªçc text r·ªóng
                valid_texts = [(idx, text) for idx, text in enumerate(clean_texts) if text]
                
                if not valid_texts:
                    failed_indices.extend(range(i, i + len(batch_texts)))
                    continue
                
                try:
                    # G·ªçi API cho batch
                    response = self.client.embeddings.create(
                        input=[text for _, text in valid_texts],
                        model=self.model
                    )
                    
                    # L∆∞u k·∫øt qu·∫£
                    for j, (original_idx, text) in enumerate(valid_texts):
                        embedding = response.data[j].embedding
                        token_count = self._count_tokens(text)
                        total_tokens += token_count
                        
                        all_embeddings.append({
                            "index": i + original_idx,
                            "embedding": embedding,
                            "token_count": token_count,
                            "text_length": len(text)
                        })
                    
                    # Rate limiting
                    time.sleep(0.1)  # Tr√°nh hit rate limit
                    
                except Exception as batch_error:
                    print(f"L·ªói batch {i}-{i+batch_size}: {batch_error}")
                    failed_indices.extend(range(i, i + len(batch_texts)))
            
            # C·∫≠p nh·∫≠t usage stats
            self.usage_stats["total_tokens"] += total_tokens
            self.usage_stats["total_requests"] += len(texts) - len(failed_indices)
            self.usage_stats["total_cost"] += (total_tokens / 1000) * self.model_info["cost_per_1k"]
            
            return {
                "success": len(all_embeddings) > 0,
                "embeddings": all_embeddings,
                "total_processed": len(all_embeddings),
                "total_failed": len(failed_indices),
                "failed_indices": failed_indices,
                "total_tokens": total_tokens,
                "model": self.model,
                "created_at": datetime.utcnow()
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": f"L·ªói khi t·∫°o batch embeddings: {str(e)}"
            }
    
    def chunk_and_embed(self, text: str, chunk_size_tokens: int = None, overlap_tokens: int = 50) -> Dict[str, Any]:
        """
        Chia text th√†nh chunks v√† t·∫°o embedding cho t·ª´ng chunk
        
        Args:
            text (str): Text c·∫ßn x·ª≠ l√Ω
            chunk_size_tokens (int): K√≠ch th∆∞·ªõc chunk (tokens)
            overlap_tokens (int): Overlap gi·ªØa chunks
            
        Returns:
            Dict[str, Any]: K·∫øt qu·∫£ chunks v√† embeddings
        """
        try:
            # L√†m s·∫°ch text
            clean_text = self._clean_text(text)
            if not clean_text:
                return {
                    "success": False,
                    "error": "Text r·ªóng sau khi l√†m s·∫°ch"
                }
            
            # Chia th√†nh chunks
            if chunk_size_tokens is None:
                chunk_size_tokens = self.model_info["max_tokens"] - 100
            
            chunks = self._split_text_by_tokens(clean_text, chunk_size_tokens, overlap_tokens)
            
            if not chunks:
                return {
                    "success": False,
                    "error": "Kh√¥ng th·ªÉ chia text th√†nh chunks"
                }
            
            # T·∫°o embeddings cho t·ª´ng chunk
            chunk_embeddings = []
            total_tokens = 0
            
            for i, chunk in enumerate(chunks):
                result = self.create_embedding(chunk)
                
                if result["success"]:
                    chunk_embeddings.append({
                        "chunk_index": i,
                        "content": chunk,
                        "embedding": result["embedding"],
                        "token_count": result["token_count"],
                        "text_length": len(chunk),
                        "start_position": clean_text.find(chunk[:50]),  # ∆Ø·ªõc t√≠nh v·ªã tr√≠
                    })
                    total_tokens += result["token_count"]
                else:
                    print(f"L·ªói embedding chunk {i}: {result['error']}")
            
            return {
                "success": len(chunk_embeddings) > 0,
                "original_text": clean_text,
                "total_chunks": len(chunks),
                "successful_chunks": len(chunk_embeddings),
                "chunks": chunk_embeddings,
                "total_tokens": total_tokens,
                "model": self.model,
                "chunk_size_tokens": chunk_size_tokens,
                "overlap_tokens": overlap_tokens,
                "created_at": datetime.utcnow()
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": f"L·ªói khi chunk v√† embed: {str(e)}"
            }
    
    def calculate_similarity(self, embedding1: List[float], embedding2: List[float]) -> float:
        """
        T√≠nh cosine similarity gi·ªØa 2 embeddings
        
        Args:
            embedding1 (List[float]): Vector embedding 1
            embedding2 (List[float]): Vector embedding 2
            
        Returns:
            float: Cosine similarity (0-1)
        """
        try:
            import math
            
            # T√≠nh dot product
            dot_product = sum(a * b for a, b in zip(embedding1, embedding2))
            
            # T√≠nh magnitude
            mag1 = math.sqrt(sum(a * a for a in embedding1))
            mag2 = math.sqrt(sum(b * b for b in embedding2))
            
            if mag1 == 0 or mag2 == 0:
                return 0.0
            
            # Cosine similarity
            similarity = dot_product / (mag1 * mag2)
            
            # ƒê·∫£m b·∫£o k·∫øt qu·∫£ trong kho·∫£ng [0, 1]
            return max(0.0, min(1.0, similarity))
            
        except Exception as e:
            print(f"L·ªói khi t√≠nh similarity: {e}")
            return 0.0
    
    def get_usage_stats(self) -> Dict[str, Any]:
        """
        L·∫•y th·ªëng k√™ s·ª≠ d·ª•ng
        
        Returns:
            Dict[str, Any]: Th·ªëng k√™ usage
        """
        return {
            **self.usage_stats,
            "model": self.model,
            "cost_per_1k_tokens": self.model_info["cost_per_1k"],
            "max_tokens_per_request": self.model_info["max_tokens"],
            "embedding_dimensions": self.model_info["dimensions"]
        }
    
    def create_text_hash(self, text: str) -> str:
        """
        T·∫°o hash ƒë·ªÉ identify text unique
        
        Args:
            text (str): Text c·∫ßn hash
            
        Returns:
            str: MD5 hash
        """
        return hashlib.md5(text.encode('utf-8')).hexdigest()

# Example usage
if __name__ == "__main__":
    # Demo usage (c·∫ßn OpenAI API key)
    try:
        embedding_tool = EmbeddingTool()
        
        print("=== Embedding Tool Demo ===")
        
        # Test text
        test_text = """
        ƒê√¢y l√† m·ªôt ƒëo·∫°n vƒÉn b·∫£n test ƒë·ªÉ demo Embedding Tool.
        Tool n√†y c√≥ th·ªÉ t·∫°o vector embeddings t·ª´ text s·ª≠ d·ª•ng OpenAI API.
        N√≥ h·ªó tr·ª£ chia text th√†nh chunks v√† x·ª≠ l√Ω batch.
        """
        
        # T·∫°o embedding ƒë∆°n
        result = embedding_tool.create_embedding(test_text)
        if result["success"]:
            print(f"‚úÖ Embedding th√†nh c√¥ng:")
            print(f"   Dimensions: {result['dimensions']}")
            print(f"   Tokens: {result['token_count']}")
            print(f"   Text length: {result['text_length']}")
        else:
            print(f"‚ùå L·ªói: {result['error']}")
        
        # Test chunk v√† embed
        long_text = test_text * 10  # T·∫°o text d√†i
        chunk_result = embedding_tool.chunk_and_embed(long_text, chunk_size_tokens=100)
        
        if chunk_result["success"]:
            print(f"\n‚úÖ Chunk v√† embed th√†nh c√¥ng:")
            print(f"   Total chunks: {chunk_result['total_chunks']}")
            print(f"   Successful chunks: {chunk_result['successful_chunks']}")
            print(f"   Total tokens: {chunk_result['total_tokens']}")
        
        # Usage stats
        stats = embedding_tool.get_usage_stats()
        print(f"\nüìä Usage Stats:")
        print(f"   Total requests: {stats['total_requests']}")
        print(f"   Total tokens: {stats['total_tokens']}")
        print(f"   Estimated cost: ${stats['total_cost']:.6f}")
        
    except Exception as e:
        print(f"Demo kh√¥ng th·ªÉ ch·∫°y: {e}")
        print("C·∫ßn c√†i ƒë·∫∑t OpenAI API key v√† c√°c dependencies")
